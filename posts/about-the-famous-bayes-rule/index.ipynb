{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To warm up for future posts about Bayesian Statistics, let's review the famous Bayes formula and other basic concepts in probability.\n",
    "\n",
    "The Bayes formula is _beautiful_ in the truest sense of this word. This simple formula governs the entire Bayesian inference world, whenever we want to guess the value of an unknown term $x$ given our knowledge about another known (already measured) term $z$:\n",
    "\n",
    "$$ p(x|z) = \\frac{p(z|x)p(x)}{p(z)} $$\n",
    "\n",
    "This simple Bayes rule can answer everything, from the simplest question like: \"I see the grass is wet. Was it raining yesterday?\", to the most complicated problem, such as: \"She is smiling to me. Does she actually love me???\" Of course, these are just silly motivation examples. Bayes rule is actually used in 99.999% applications we see today in Google search, cars, airplanes, sattelites, to Mars Rovers and Curiosity... Ok, I lied. I don't know the exact number. But anyway...\n",
    "\n",
    "However, there are actually a lot going on in that simple formula. Each term has a different name, although they all look like \"propability density\" functions $p(\\cdot)$: \n",
    "- the posterior probability $p(x|z)$\n",
    "- the likelihood function $p(z|x)$\n",
    "- the prior probability $p(x)$\n",
    "- and the \"partition function\" or \"normalization constant\" $p(z)$\n",
    "\n",
    "It took me quite a while to grab the meaning of those terms. Here are several questions I had in mind when I first try to understand them. If you think you can answer them, think twice! However, if you are among the normal 90% as I am, I bet there are some subtleties explained here that you will find interesting.\n",
    "\n",
    "0. Why do we need to compute the full posterior density $p(x|z)$? We just need to infer the one best value of $x$ to make decision, don't we? \n",
    "1. Why is $p(z|x)$ called a \"likelihood function\"? Is it just a \"conditional probability\"? What is the difference between a \"likelihood function\" and a \"conditional probability\"?\n",
    "2. What is a _conditional probability_ anyway? If we think the conditional probability density function $p(a|b)$ as a function of two variables $a$ and $b$, how is it different from the _joint density_ $p(a,b)$, which is also a function of two variables? \n",
    "3. Is the prior $p(x)$ important? Why can't we just assume it a uniform distribution every time?\n",
    "4. What does it mean to say that $p(z)$ is a normalization constant? Is it not a density function?\n",
    "\n",
    "I told you, there are a lot going on in that simple formula. At this point, I realize that each question is actually worth a post for itself. Stay tunned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
